---
slug: 2025-06-20-why-human-intent-matters-more-as-ai-capabilities-grow.md
title: 为什么随着 AI 能力的提升，人类意图变得更加重要
authors: [sleepy]
tags: [AI, Cline]
---

# 为什么随着 AI 能力的提升，人类意图变得更加重要

![](https://fastly.jsdelivr.net/gh/bucketio/img17@main/2025/06/19/1750328660936-1ea0145a-8060-45e6-8641-5e2879d98704.png)

发烧两天，终于恢复，继续更新。

我发现 Cline 的官方博客真的是宝藏，虽然其实很多博客都是在宣传自身或者自身的一个 feature，但确实有很多很深刻又很有启发的观点，忍不住想分享给大家。比如今天这篇：[**为什么随着 AI 能力的提升，人类意图变得更加重要。**](https://cline.bot/blog/why-human-intent-matters-more-as-ai-capabilities-grow)配合前文[
Cline 源码浅析 - Prompt 设计
](https://mp.weixin.qq.com/s/pdznzYiS6oUT1epOuBLX3g)食用更加。

---

还记得当 AI 编码只是简单的自动补全吗？那时，人类完成了大部分工作：浏览代码库、寻找正确的文件、定位要编辑的位置、开始输入，然后 AI 才能提供一些建议。人类是主导者，而 AI 只是辅助。

如今的智能 AI 能够搜索代码库、读取文件、编写完整模块、重构系统，并在多个文件中进行复杂的变更。借助像 Cline 这样的工具，AI 几乎接管了整个编码过程。这就产生了一个有趣的问题：如果 AI 几乎能做所有事情，人类还能做什么？

答案其实很简单：**意图**。

## 最后一段路是最重要的

随着 AI 能力的增强，人类的角色并没有减少，而是发生了转变。当 AI 能够处理编码的细节时，我们需要思考：*我们在构建什么，以及为什么要构建它？*

![](https://fastly.jsdelivr.net/gh/bucketio/img5@main/2025/06/19/1750328832364-a318ee62-8982-441b-99b0-04f0d65594f3.png)

可以将这过程视为一张工作量的饼图。在自动补全时代：

* 人类工作量： 95%（导航、规划、输入、调试）
* AI 工作量：5%（提供建议）

在智能 AI 时代：

* AI 工作量：95%（搜索、阅读、写作、重构）
* 人类工作量：5%（意图）

而这 5% 就是**决定性因素**。它决定了 AI 是完美地构建错误的东西，还是构建完全符合需求的东西。

## 为什么我们创建了计划-行动模式

问题在于：AI 充满热情。一旦赋予它读写文件的工具，它就会立即开始编码。这就像一个才华横溢的开发者在完全理解需求之前就开始动手。

这种急切并不是缺陷，而是这些模型工作的特性。它们被设计为提供帮助、采取行动、解决问题。但如果没有明确的意图，行动可能会走偏。

计划-行动模式不是为了给 AI 设置不同的“模式”，而是为了认识到有效合作的自然阶段。在计划阶段，我们为相互理解创造了一个专门的空间。我们让 AI 尽可能地关注人类的意图。不是因为 AI 能力不足，而是因为意图是唯一无法自动化的东西。

接下来是行动阶段，我们将其放手。AI 的能力不变，但现在优化为不间断的执行。这是我们互动方式的模式转变，而不是性格的变化。

## 反角色哲学 (为何计划-行动不是“模式”)

你可能见过一些编码助手，拥有专门的“智能体”：比如调试器、架构师和代码审阅者。我们刻意避开这种方法，原因如下图：

![我们设计 Cline 是为了展现模型的潜力，而不是按照我们的需求来操控它们。](https://fastly.jsdelivr.net/gh/bucketio/img3@main/2025/06/19/1750340787740-6044e41d-6a0e-4eee-871c-3d3cfadbf460.png)

> 图中的英文翻译：第二个要从这个痛苦的教训中学到的要点是：心灵的实际内容是极其复杂的，无法挽回的；我们应该停止尝试找到简单的方法来思考心灵的内容，比如思考空间、物体、多个智能体或对称性的简单方法。所有这些都是任意的、本质上复杂的外界的一部分。由于它们的复杂性是无穷的，因此不应内置其中；相反，**我们应该仅构建能够发现和捕捉这种任意复杂性的元方法。**

Richard Sutton 在 AI 研究中的[“苦涩教训”](http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=cline.ghost.io)指出，利用计算的通用方法往往优于专业化的人工设计解决方案。当深蓝击败 Kasparov 时，它并没有使用大师级策略，而是依靠蛮力搜索。当 AlphaGo 征服围棋时，它并没有遵循古老的智慧，而是使用通用学习算法。

现代语言模型，例如 Claude Sonnet 4 和 Geminin 2.5 Pro，经过完整的编码过程训练，已经见识过数百万个架构决策、调试会话和代码审阅的案例。人工角色的创建会限制它们自然的能力。

这点很关键：**计划和行动不是不同 AI 个性或能力的“模式”**。我们没有在“计划 AI”和“编码 AI”之间切换。它始终是同一个 AI，能力不变。

变化的是交互模式。在计划阶段，我们优化意图收集；AI 会提问，探索上下文，确保理解。在行动阶段，我们优化执行；AI 不间断地发挥其全部能力。这是关于识别人类与 AI 合作的自然节奏，而不是限定 AI 的角色。

可以将其类比于对话与专注工作。你不会认为开发者在需求会议与编码时有不同的“模式”。他们是同一个人，只是在不同的工作阶段中。

## 不要打扰 AI

我们发现一个有趣的现象：大语言模型是出色的[叙述者](https://x.com/cline/status/19107658728280802930?ref=cline.ghost.io)。它们是为构建连贯故事而设计的预测文本引擎：先发生这个，然后是那个，再然后是另一个。

如果你在 AI 执行任务时中途打断它来纠正方向，就会破坏这种叙述的流畅性。模型需要调整你的干预与之前的路径，通常会导致混乱或不理想的结果。

斯坦福大学的[“迷失在中间”](https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf?ref=cline.ghost.io)研究 (Liu et al., 2023) 表明，当处理被打断的上下文时，大语言模型的性能会显著下降。研究发现，当相关信息出现在长上下文的中间而不是开始时，模型的准确率下降了近 20%。重新开始，虽然看似低效，但通常效果更好，因为模型能够从清晰的意图构建完整的叙述，而不需要在受损的中间上下文信息中挣扎。

这就是为什么重新开始并提供更清晰的指令通常比纠正方向更有效。保存当前状态，让 AI 继续工作，然后接受结果或者重新开始，而不是纠结于修正。

随着推理成本下降，我们甚至可以同时进行多次尝试，并选择最佳结果，就像拉动老虎机一样，每次拉杆的成功概率都是独立的。

## 让 AI 自主运作

在计划阶段确定了你的意图后，接下来的原则同样重要：让 AI 自由发挥。

这与我们的反角色哲学直接相关。我们不将 AI 限制为“调试器”或“架构师”等特定角色，同样，我们也不会在执行过程中限制它。行动阶段的重点是让模型充分发挥其所有能力：架构决策、调试、重构、优化，作为一个整体智能体。

我们注意到，最新的 Claude 4 模型在自动批准模式下表现优异。每次中断都会打断其叙述流程。最有效的情况是 AI 能够不受干扰地进行写作。

> 需要注意的是，模型的最佳表现来自不受干扰的构建。然而，结果可能并不总是与你的意图完全一致。因此，如果你从事复杂的工程工作，我们建议你仍然保持参与，更重要的是，详细制定你的计划。

这可能看起来有些违反直觉。难道我们不需要控制、监督和干预的能力吗？

是的，但不需要实时进行。这就是我们构建以下功能的原因：

* 检查点（Checkpoints）：保存状态，随时可以返回。
* 全面可见性（Full visibility）：了解 AI 的全部操作。
* 撤销功能（Undo capabilities）：撤销任何更改。
* 安全边界（Safety boundaries）：定义 AI 能触及和不能触及的范围。

关键在于时机。先收集需求，共享意图，设定边界，然后让 AI 工作。之后再进行审查和迭代，而不是实时进行。

---

编码的未来不在于人类与 AI 竞争或通过不断细化专业领域来保持重要性。而在于人类发挥独特的作用：提供意图、意义和目标，而由 AI 来执行具体操作。

这不仅不是一个减弱的角色，而是所有角色中最为重要的。

---

<div align="center">
  <p>欢迎关注我的公众号：前端生存指南，一起聊聊前端、AI 和生活。</p>
  <img src="https://cloud-minapp-47803.cloud.ifanrusercontent.com/1tvAM68Cvrx3bfLR.jpg" style={{ width: '180px' }} />
</div>